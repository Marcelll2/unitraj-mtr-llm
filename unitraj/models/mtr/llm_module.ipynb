{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import Necessary Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from peft import LoftQConfig, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"F:\\\\FAU_programm\\\\Master_Arbeit\\\\UniTraj\\\\UniTraj_llm\\\\unitraj\\\\models\\\\llm_cache\"\n",
    "model = GPT2Model.from_pretrained(pretrained_model_name_or_path='gpt2',\n",
    "                                  cache_dir=cache_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path='gpt2', \n",
    "                                          cache_dir=cache_path)\n",
    "# loftq_config = LoftQConfig(loftq_bits=4)           # set 4bit quantization\n",
    "lora_config = LoraConfig(init_lora_weights=\"gaussian\")\n",
    "# lora_config = LoraConfig(init_lora_weights=\"loftq\", loftq_config=loftq_config)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer.encode(\"What is the capital of France?\", return_tensors=\"pt\")\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *last hidden state & past key values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output的past_key_values代表transformer每一层的key和value的输出\n",
    "\n",
    "形状是((B, H, N, C), (B, H, N, C))，-> (keys, values)\n",
    "\n",
    "B是batch size，H是head数量，N是序列长度，C是特征维度\n",
    "\n",
    "总共有层数L=12个((B, H, N, C), (B, H, N, C))的组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['last_hidden_state', 'past_key_values']),\n",
       " 12,\n",
       " 2,\n",
       " torch.Size([1, 12, 7, 64]),\n",
       " torch.Size([1, 12, 7, 64]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys(), len(output['past_key_values']), len(output['past_key_values'][0]), output['past_key_values'][0][0].shape, output['past_key_values'][0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = torch.randn((2, 900, 768), dtype=torch.float16)\n",
    "output = model(inputs_embeds=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['last_hidden_state', 'past_key_values']),\n",
       " torch.Size([2, 900, 768]),\n",
       " 12,\n",
       " torch.Size([2, 12, 900, 64]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys(), output['last_hidden_state'].shape, len(output['past_key_values']), output['past_key_values'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    filename='test.log',\n",
    "                    filemode='a')\n",
    "logging.info(f'This is a log message\\n'\n",
    "             f'This is a log message\\n'\n",
    "             f'This is a log message\\n'\n",
    "             f'This is a log message1212')\n",
    "logging.info('This is a log message')\n",
    "logging.info('This is a log message')\n",
    "logging.info('1')\n",
    "logging.info('1')\n",
    "logging.info('End')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unitraj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
